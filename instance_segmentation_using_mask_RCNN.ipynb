{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing instance segmentation using Mask R-CNN\n",
    "The Mask R-CNN architecture helps in identifying / highlighting the instances of objects of a given class within an image. This comes in especially handy when there are multiple objects of the same type present within the image. Furthermore, the term Mask represents the segmentation that's done at the pixel level by Mask R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from torch_snippets import *\n",
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perpairint the data and importing the relevant dataset and training utilities from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/images.tar\n",
    "!wget --quiet http://sceneparsing.csail.mit.edu/data/ChallengeData2017/annotations_instance.tar\n",
    "!tar -xf images.tar\n",
    "!tar -xf annotations_instance.tar\n",
    "!rm images.tar annotations_instance.tar\n",
    "!pip install -qU torch_snippets\n",
    "!wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/engine.py\n",
    "!wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/utils.py\n",
    "!wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/transforms.py\n",
    "!wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/coco_eval.py\n",
    "!wget --quiet https://raw.githubusercontent.com/pytorch/vision/master/references/detection/coco_utils.py\n",
    "!pip install -q -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = Glob('images/training')\n",
    "all_annots = Glob('annotations_instance/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'ADE_train_00014301'\n",
    "\n",
    "im = read(find(f, all_images), 1)\n",
    "an = read(find(f, all_annots), 1).transpose(2,0,1)\n",
    "r,g,b = an\n",
    "nzs = np.nonzero(r==4) # 4 stands for person\n",
    "instances = np.unique(g[nzs])\n",
    "masks = np.zeros((len(instances), *r.shape))\n",
    "for ix,_id in enumerate(instances):\n",
    "    masks[ix] = g==_id\n",
    "\n",
    "subplots([im, *masks], sz=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots = []\n",
    "\n",
    "for ann in Tqdm(all_annots):\n",
    "    _ann = read(ann, 1).transpose(2,0,1)\n",
    "    r,g,b = _ann\n",
    "    if 4 not in np.unique(r): continue\n",
    "    annots.append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_annots = stems(annots)\n",
    "trn_items, val_items = train_test_split(_annots, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    \n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    \n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        \n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasksDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, items, transforms, N):\n",
    "        \n",
    "        self.items = items\n",
    "        self.transforms = transforms\n",
    "        self.N = N\n",
    "        \n",
    "    def get_mask(self, path):\n",
    "        \n",
    "        an = read(path, 1).transpose(2,0,1)\n",
    "        r,g,b = an\n",
    "        nzs = np.nonzero(r==4)\n",
    "        instances = np.unique(g[nzs])\n",
    "        masks = np.zeros((len(instances), *r.shape))\n",
    "        \n",
    "        for ix,_id in enumerate(instances):\n",
    "            masks[ix] = g==_id\n",
    "            \n",
    "        return masks\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        \n",
    "        _id = self.items[ix]\n",
    "        img_path = f'images/training/{_id}.jpg'\n",
    "        mask_path = f'annotations_instance/training/{_id}.png'\n",
    "        masks = self.get_mask(mask_path)\n",
    "        obj_ids = np.arange(1, len(masks)+1)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            obj_pixels = np.where(masks[i])\n",
    "            xmin = np.min(obj_pixels[1])\n",
    "            xmax = np.max(obj_pixels[1])\n",
    "            ymin = np.min(obj_pixels[0])\n",
    "            ymax = np.max(obj_pixels[0])\n",
    "            \n",
    "            if (((xmax-xmin)<=10) | (ymax-ymin)<=10):\n",
    "                xmax = xmin+10\n",
    "                ymax = ymin+10\n",
    "                \n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        image_id = torch.tensor([ix])\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def choose(self):\n",
    "        return self[randint(len(self))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = MasksDataset(trn_items, get_transform(train=True), N=100)\n",
    "\n",
    "im,targ = x[0]\n",
    "inspect(im,targ)\n",
    "\n",
    "subplots([im, *targ['masks']], sz=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the instance segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    \n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the dataset and dataloaders that correspond to the train and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MasksDataset(trn_items, get_transform(train=True), N=3000)\n",
    "dataset_test = MasksDataset(val_items, get_transform(train=False), N=800)\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the model, parameters, and optimization criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes).to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The defined pre-trained model architecture takes the image and the targets dictionary as input to reduce loss.\n",
    "# output sample:\n",
    "model.eval()\n",
    "pred = model(dataset[0][0][None].to(device))\n",
    "inspect(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fetch the number of instances that have been detected\n",
    "pred[0]['masks'].shape\n",
    "torch.Size([100, 1, 536, 559])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model over increasing epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "trn_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    res = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    trn_history.append(res)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    res = evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "losses = [np.mean(list(trn_history[i].meters['loss'].deque)) for i in range(len(trn_history))]\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "im = dataset_test[0][0]\n",
    "show(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    prediction = model([im.to(device)])\n",
    "    \n",
    "    for i in range(len(prediction[0]['masks'])):\n",
    "        plt.imshow(Image.fromarray(prediction[0]['masks'][i, 0].mul(255).byte().cpu().numpy()))\n",
    "        plt.title('Class: '+str(prediction[0]['labels'][i].cpu().numpy())+' Score:'+str(prediction[0]['scores'][i].cpu().numpy()))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
